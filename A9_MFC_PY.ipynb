{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <b>Normal and Accelerated Gradient Descent in Logistic Regression </b>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "### Group - A9\n",
    "\n",
    "### Team Members\n",
    "\n",
    "- Diya Deepak (CB.SC.U4AIE23020)\n",
    "- Ghanasree S (CB.SC.U4AIE23028)\n",
    "- Neha Jacob (CB.SC.U4AIE23046)\n",
    "- Sriranjana C (CB.SC.U4AIE23066)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> OBJECTIVE </b>\n",
    "- Take a ML problem (Logistic Regression)\n",
    "- Optimize its cost function using Gradient Descent Optimization (with and without momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "x644ebsFwoLG",
    "outputId": "b9f9c7e1-5b25-4ecd-99cb-82601b0482e7"
   },
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Problem Statement </b>\n",
    "\n",
    "Predict student admissions based on their performance in JEE and Amrita exams using logistic regression\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading the dataset \n",
    "- Separating the Features and Target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "admission_data = pd.read_csv('Logistic_Regression_Dataset_with_Amrita_and_JEE_Scores.csv')\n",
    "X = admission_data[['Amrita_Score', 'JEE_Score']].values     # Features / Independent variable\n",
    "y = admission_data['Admitted'].values    # Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalizing the feature values for a better performance\n",
    "- Splitting the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split the normalized data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> LOGISTIC REGRESSION </b>\n",
    "\n",
    "A statistical method used for binary classification problems, where the goal is to predict one of two possible outcomes\n",
    "\n",
    "- The probability is mapped to one of two classes using a threshold (commonly 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid function \n",
    "\n",
    "- Maps any real-valued input into a range between 0 and 1\n",
    "- Used in logistic regression to convert raw predictions into probabilities\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with Gradient Descent (without momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function for Logistic Regression\n",
    "\n",
    "$$\n",
    "J(w, b) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m $ is the number of training examples.\n",
    "- $ h_\\theta(x^{(i)}) = \\frac{1}{1 + e^{-z}} $ is the sigmoid function applied to the linear model.\n",
    "- $ y^{(i)} $ represents the true label for the $i$-th example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the cost function \n",
    "def compute_cost(y_true, y_predicted):\n",
    "    m = len(y_true)\n",
    "    cost = - (1 / m) * np.sum(y_true * np.log(y_predicted) + (1 - y_true) * np.log(1 - y_predicted))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model with gradient descent \n",
    "def logistic_regression_gradient_descent(X, y, learning_rate=0.1, iters=1001):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    for iter in range(iters):\n",
    "        linear_model = np.dot(X, weights) + bias\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "        \n",
    "        # Compute gradients\n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "        db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "        \n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "        \n",
    "        # Calculate and print cost every 200 iteration\n",
    "        if iter % 200 == 0:\n",
    "            cost = compute_cost(y, y_predicted)\n",
    "            print(f\"Iteration {iter}: Cost = {cost}\")\n",
    "    \n",
    "    return weights, bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent without momentum\n",
    "\n",
    "$$\n",
    "h_\\theta(X) = \\sigma(z) = \\frac{1}{1 + e^{-z}}, \n",
    "$$\n",
    "\n",
    "The gradients for logistic regression are defined as:\n",
    "\n",
    "- **Weight Gradient**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "- **Bias Gradient**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The parameters are updated iteratively using the following equations:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial J}{\\partial w_j}, \\quad b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha\\ $ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 0.6931471805599452\n",
      "Iteration 200: Cost = 0.23834728681623155\n",
      "Iteration 400: Cost = 0.2224416408276094\n",
      "Iteration 600: Cost = 0.21764164622079876\n",
      "Iteration 800: Cost = 0.21565674681567007\n",
      "Iteration 1000: Cost = 0.21471244751371274\n",
      "Iteration 1200: Cost = 0.21422645211080185\n",
      "Iteration 1400: Cost = 0.21396367328961816\n",
      "Iteration 1600: Cost = 0.21381680759767505\n",
      "Iteration 1800: Cost = 0.2137328025259781\n",
      "Iteration 2000: Cost = 0.2136839447621258\n",
      "Iteration 2200: Cost = 0.21365517821428845\n",
      "Iteration 2400: Cost = 0.2136380852375762\n",
      "Iteration 2600: Cost = 0.21362785822758681\n",
      "Iteration 2800: Cost = 0.21362170697034435\n",
      "Iteration 3000: Cost = 0.21361799222737166\n",
      "Iteration 3200: Cost = 0.213615741927752\n",
      "Iteration 3400: Cost = 0.21361437548228146\n",
      "Iteration 3600: Cost = 0.21361354419659193\n",
      "Iteration 3800: Cost = 0.213613037749399\n",
      "Iteration 4000: Cost = 0.21361272885870017\n",
      "\n",
      "Model Accuracy without Momentum: 0.915\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression model using gradient descent without momentum\n",
    "weights, bias = logistic_regression_gradient_descent(X_train, y_train, learning_rate=0.2, iters=4001)\n",
    "\n",
    "# Define prediction function\n",
    "def predict(X, weights, bias, threshold=0.5):\n",
    "    linear_model = np.dot(X, weights) + bias\n",
    "    y_predicted = sigmoid(linear_model)\n",
    "    return [1 if i > threshold else 0 for i in y_predicted]\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = predict(X_test, weights, bias)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print()\n",
    "print(\"Model Accuracy without Momentum:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input - JEE Score: 23.0, Amrita Exam Score: 45.0\n",
      "Oops..!! Admission is not likely.\n"
     ]
    }
   ],
   "source": [
    "# Function to predict admission based on user input\n",
    "def predict_admission():\n",
    "    # Get user input\n",
    "    jee_score = float(input(\"Enter your JEE score: \"))\n",
    "    exam_score = float(input(\"Enter your Amrita exam score: \"))\n",
    "\n",
    "    # Check if both scores are less than 100\n",
    "    if jee_score >= 100 or exam_score >= 100:\n",
    "        print(\"Error: Both scores must be less than 100!\")\n",
    "        return\n",
    "    \n",
    "    # Print user input\n",
    "    print(f\"User Input - JEE Score: {jee_score}, Amrita Exam Score: {exam_score}\")\n",
    "    \n",
    "    # Normalize the user input using the same scaler\n",
    "    user_input = np.array([[jee_score, exam_score]])\n",
    "    user_input_normalized = scaler.transform(user_input)\n",
    "    \n",
    "    # Make prediction\n",
    "    admission_prediction = predict(user_input_normalized, weights, bias)\n",
    "    \n",
    "    # Print result\n",
    "    if admission_prediction[0] == 1:\n",
    "        print(\"Yayy..!! Admission is likely!\")\n",
    "    else:\n",
    "        print(\"Oops..!! Admission is not likely.\")\n",
    "    \n",
    "# Call the function to predict admission based on user input\n",
    "predict_admission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <hr> </b>\n",
    "### <b> Gradient Descent with Momentum </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "admission_data = pd.read_csv('Logistic_Regression_Dataset_with_Amrita_and_JEE_Scores.csv')\n",
    "X = admission_data[['Amrita_Score', 'JEE_Score']].values     # Features / Independent variable\n",
    "y = admission_data['Admitted'].values    # Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split the normalized data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Momentum\n",
    "\n",
    "- In gradient descent with momentum, we introduce a velocity term that helps accelerate convergence and smooth out the oscillations in the optimization path. \n",
    "- The update rules for the weights and bias include a momentum term, which is a moving average of the gradients.\n",
    "\n",
    "- Momentum parameter is denoted by $\\beta$, where \\( 0 $\\leq$ $\\beta$ < 1 \\). \n",
    "- This parameter controls how much of the previous gradient is retained.\n",
    "\n",
    "#### Velocity Update Rules\n",
    "\n",
    "- **Velocity Update (for weights):**\n",
    "\n",
    "$$\n",
    "v_j = \\beta v_j + (1 - \\beta) \\frac{\\partial J}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "- **Velocity Update (for bias):**\n",
    "\n",
    "$$\n",
    "v_b = \\beta v_b + (1 - \\beta) \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($ v_j $\\) is the velocity for weight  \\($ w_j $\\),\n",
    "- \\( $v_b $\\) is the velocity for bias \\( b \\),\n",
    "- \\($ \\beta$ \\) is the momentum term.\n",
    "\n",
    "#### Parameter Update Rules\n",
    "\n",
    "- **Weight Update:**\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha v_j\n",
    "$$\n",
    "\n",
    "- **Bias Update:**\n",
    "\n",
    "$$\n",
    "b = b - \\alpha v_b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha $ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model with gradient descent and momentum\n",
    "def logistic_regression_gradient_descent_with_momentum(X, y, learning_rate=0.2, iters=1001, beta=0.9):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    # Initialize momentum terms\n",
    "    V_dw = np.zeros(n_features)\n",
    "    V_db = 0\n",
    "\n",
    "    for iter in range(iters):\n",
    "        linear_model = np.dot(X, weights) + bias\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "\n",
    "        # Compute gradients\n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "        db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "        # Update momentum terms\n",
    "        V_dw = beta * V_dw + (1 - beta) * dw\n",
    "        V_db = beta * V_db + (1 - beta) * db\n",
    "\n",
    "        # Update weights and bias with momentum\n",
    "        weights -= learning_rate * V_dw\n",
    "        bias -= learning_rate * V_db\n",
    "\n",
    "        # Calculate and print cost every 200 iterations\n",
    "        if iter % 200 == 0:\n",
    "            cost = compute_cost(y, y_predicted)\n",
    "            print(f\"Iteration {iter}: Cost = {cost}\")\n",
    "\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 0.6931471805599452\n",
      "Iteration 200: Cost = 0.23570849354744824\n",
      "Iteration 400: Cost = 0.2215539090463947\n",
      "Iteration 600: Cost = 0.21724239138429027\n",
      "Iteration 800: Cost = 0.2154525316791497\n",
      "Iteration 1000: Cost = 0.2146003421555841\n",
      "Iteration 1200: Cost = 0.21416218038023158\n",
      "Iteration 1400: Cost = 0.21392576234997848\n",
      "Iteration 1600: Cost = 0.21379400807644758\n",
      "Iteration 1800: Cost = 0.2137189041605106\n",
      "Iteration 2000: Cost = 0.21367539089311627\n",
      "Iteration 2200: Cost = 0.2136498776191374\n",
      "Iteration 2400: Cost = 0.21363478462871094\n",
      "Iteration 2600: Cost = 0.21362579594078962\n",
      "Iteration 2800: Cost = 0.21362041535702894\n",
      "Iteration 3000: Cost = 0.21361718200209553\n",
      "Iteration 3200: Cost = 0.2136152331630869\n",
      "Iteration 3400: Cost = 0.21361405582966178\n",
      "Iteration 3600: Cost = 0.21361334331038634\n",
      "Iteration 3800: Cost = 0.2136129115000854\n",
      "Iteration 4000: Cost = 0.21361264952843087\n",
      "\n",
      "Model Accuracy with Momentum: 0.915\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression model using gradient descent with momentum\n",
    "weights, bias = logistic_regression_gradient_descent_with_momentum(X_train, y_train, learning_rate=0.2, iters=4001, beta=0.9)\n",
    "\n",
    "# Define prediction function\n",
    "def predict(X, weights, bias, threshold=0.5):\n",
    "    linear_model = np.dot(X, weights) + bias\n",
    "    y_predicted = sigmoid(linear_model)\n",
    "    return [1 if i > threshold else 0 for i in y_predicted]\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = predict(X_test, weights, bias)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print()\n",
    "print(\"Model Accuracy with Momentum:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input - JEE Score: 45.0, Amrita Exam Score: 56.0\n",
      "Admission is not likely.\n"
     ]
    }
   ],
   "source": [
    "# Define prediction function with user input and validation\n",
    "def predict_admission_with_momentum():\n",
    "    # Get user input\n",
    "    \n",
    "    jee_score = float(input(\"Enter your JEE score: \"))\n",
    "    exam_score = float(input(\"Enter your Amrita exam score: \"))\n",
    "\n",
    "    # Check if both scores are less than 100\n",
    "    if jee_score >= 100 or exam_score >= 100:\n",
    "        print(\"Error: Both scores must be less than 100!\")\n",
    "        return\n",
    "    \n",
    "    # Print user input\n",
    "    print(f\"User Input - JEE Score: {jee_score}, Amrita Exam Score: {exam_score}\")\n",
    "    \n",
    "    # Normalize the user input using the same scaler\n",
    "    user_input = np.array([[jee_score, exam_score]])\n",
    "    user_input_normalized = scaler.transform(user_input)\n",
    "    \n",
    "    # Make prediction\n",
    "    admission_prediction = predict(user_input_normalized, weights, bias)\n",
    "    \n",
    "    # Print result\n",
    "    if admission_prediction[0] == 1:\n",
    "        print(\"Admission is likely!\")\n",
    "    else:\n",
    "        print(\"Admission is not likely.\")\n",
    "\n",
    "# Call the function to predict admission based on user input\n",
    "predict_admission_with_momentum()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
